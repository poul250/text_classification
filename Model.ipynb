{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain, groupby\n",
    "from random import choice\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import typing as tp\n",
    "TCorpus = tp.List[tp.Tuple[str, str, tp.Dict[str, tp.Set[str]]]]\n",
    "TNgrams = tp.Dict[tp.Tuple[str], int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questionId': 'b5f69407-d633-400c-9c11-ea40f59912c9',\n",
       " 'userId': 'NyB5YNGjqxmsKjW3w',\n",
       " 'title': \"Facebook Auto-UnLiker — Your Facebook Page 'Likes' Might Drop This Week\",\n",
       " 'text': '\\n\\n\\n\\nDo you own a Facebook Business page? If yes, then you will notice a drop in the number of \"likes\" on your Facebook Page by next week, which could be quite disappointing but, Facebook believes, will help business to know their actual followers.\\n\\n\\n\\n\\n\\nFACEBOOK\\'S OFFICIAL MASS AUTO-UNLIKE \\n\\nThe social network giant is giving its Pages a little spring cleaning, purging them of memorialized and voluntarily deactivated inactive Facebook accounts in an attempt to make its users data more meaningful for businesses and brands.\\n\\n   \\n\\n\\nFacebook purge will begin from March 12, Facebook said, and should continue over the next few weeks.\\n\\n\"Over the coming weeks, Page admins should expect to see a small dip in their number of Page likes as a result of this update,\" Facebook said in a blog post. \"It’s important to remember, though, that these removed likes represent people who were already inactive on Facebook.\"\\n\\nFACEBOOK TO DETECT FAKE FOLLOWERS\\n\\nFacebook is also taking steps to improve how it detects fake profiles. We all know that a number of Businesses and Brands buy fake Facebook Likes and Twitter followers in order to show their brand popularity.\\n\\n\\n\\n\\nSocial Media giants, Facebook, Twitter and Google, have emerged as major players in recent general elections in India, where political parties spend millions of dollar to buy number of Followers and advertize their promo campaigns to impact Election results.\\n\\n\\n\\n\\nBENEFITS OF REMOVING INACTIVE USERS FROM LIKES\\n\\nAccording to Facebook, there are two main reasons to remove inactive Facebook accounts from Page audience:\\n\\n\\n\\nAccurate Likes\\nKeeping Actual followers on the Top\\n\\n\\nWith more accurate \"like\" counts, businesses and brands could better understand how much followers are actually interested in their contents and products. Facebook wants to give businesses “up-to-date insights” on their pages’ active followers.\\n\\n\\n\\n\\nThe move will give businesses more precise information about those Facebook users who are actively following their Facebook Page and make better use of Facebook’s Custom Audiences tool, which lets businesses create followers — aka lookalike audiences — by finding people on Facebook who are similar to those who already follow the company’s page.\\n\\n\\n\\n\\nThe company also wants to make business results consistent with individual users’ experiences. Facebook already filters out \"likes and comments generated by deactivated or memorialized accounts from individual Page posts.\"\\n\\n\\n\\n\\nWhile, the decrease in number of followers may disappoint you at the very first time, but at the same time it will help you gain a more accurate way to track your customers and grow your followers with authentic number of likes, which will be more beneficial to your business.\\n\\n\"Everyone benefits from meaningful information on Facebook. It’s our hope that this update makes Pages even more valuable for businesses,\" Facebook said.\\n',\n",
       " 'labels': ['Прочее']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"tpc_2019_01_dataset.json\") as f:\n",
    "    data = json.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data):\n",
    "    for _, group in groupby(data, key=lambda x: x[\"questionId\"]):\n",
    "        labels = {}\n",
    "        for x in group:\n",
    "            title = x[\"title\"]\n",
    "            text = x[\"text\"]\n",
    "            labels.update({x[\"userId\"]: x[\"labels\"]})\n",
    "        yield (title, text, labels)\n",
    "\n",
    "def test_data_generator(data):\n",
    "    for _, group in groupby(data, key=lambda x: x[\"questionId\"]):\n",
    "        labels = {}\n",
    "        for x in group:\n",
    "            title = x[\"title\"]\n",
    "            text = x[\"text\"]\n",
    "            labels.update({x[\"userId\"]: x[\"labels\"]})\n",
    "        yield (title, text), labels\n",
    "\n",
    "def batch_generator(data, preprocess=None, batch_size=8, shuffle=True):\n",
    "    batch = []\n",
    "    indexes = np.arange(len(data))\n",
    "    for _ in range(epochs):\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        for i in indexes:\n",
    "            batch.append(data[i])\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(\n",
    "            self,            # type: Solution\n",
    "            classes=None,    # type: tp.Optional[tp.List[str]],\n",
    "            nonclass=None,   # type: tp.Optional[str]\n",
    "            languages=None,  # type: tp.Optional[tp.Dict[str, tp.Set[str]]]\n",
    "            epochs=100,      # type: tp.Optional[int]\n",
    "            batch_size=16,   # type: tp.Optional[int]\n",
    "            gramm_size=3,    # type: tp.Optional[int]\n",
    "            verbose=0,       # type: tp.Optional[int]\n",
    "            roc_auc=0.5,     # type: tp.Optional[float]\n",
    "            lr=0.001,        # type: tp.Optional[float]\n",
    "            lr_step=0.97     # type: tp.Optional[float]\n",
    "            ):\n",
    "        # type: (...) -> None\n",
    "        self.epochs=epochs\n",
    "        self.gramm_size=gramm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = self._get_classes(classes)\n",
    "        self.nonclass = self._get_non_class(nonclass)\n",
    "        self.languages = self._get_languages(languages)\n",
    "        self.clfs = None\n",
    "        self.verbose = verbose\n",
    "        self.roc_auc = roc_auc\n",
    "        self.lr=lr\n",
    "        self.lr_step = lr_step\n",
    "\n",
    "    def train(\n",
    "            self,         # type: Solution\n",
    "            train_corpus  # type: TCorpus\n",
    "            ):\n",
    "        # type: (...) -> None\n",
    "        language_to_corpus = self._split_on_languages(train_corpus)\n",
    "        self.clfs = {\n",
    "            lang: self._train(corpus[\"corp\"], {gramm: i for i, gramm in enumerate(corpus[\"ngrams\"])}) \n",
    "            for lang, corpus in language_to_corpus.items()\n",
    "        }\n",
    "\n",
    "    def predict(\n",
    "            self,  # type: Solution\n",
    "            news   # type: List[Tuple[str, str]]\n",
    "            ):\n",
    "        # type: (...) -> List[Set[str]]\n",
    "        result = []\n",
    "        for new in news:\n",
    "            text = new[1]\n",
    "            lang = self._get_language(text)\n",
    "            n_grams, clf = self.clfs[lang]\n",
    "            vec = self._text_to_vector(text, n_grams)\n",
    "            y = clf(torch.from_numpy(vec).type(torch.FloatTensor))\n",
    "            y = {class_ for p, class_ in zip(y, self.classes) if p > self.roc_auc}\n",
    "            result.append(y if len(y) > 0 else {self.nonclass})\n",
    "        return result\n",
    "\n",
    "    def _train(\n",
    "            self,          # type: Solution\n",
    "            train_corpus,  # type: TCorpus\n",
    "            n_grams        # type: TNgrams\n",
    "            ):\n",
    "        # type: (...) -> None\n",
    "        X, Y = self._corpus_to_vectors(train_corpus, n_grams)\n",
    "\n",
    "        net = Net(len(n_grams), len(self.classes))\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        lr = self.lr\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.00001)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            for i, (X_batch, y_batch) in enumerate(self._batch_generator(X, Y)):\n",
    "                output = net(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss\n",
    "            self._verbose(epoch, epoch_loss, net, X, Y)\n",
    "\n",
    "        return n_grams, net\n",
    "    \n",
    "    def _verbose(self, epoch, loss, net, X, Y):\n",
    "        if (self.verbose > 0):\n",
    "            print(\"epoch: {}\".format(epoch), end=\" \")\n",
    "        if self.verbose > 1:\n",
    "            print(\"loss: {}\".format(loss), end=\" \")\n",
    "        if self.verbose > 2:\n",
    "            print(\"score: {}\".format(self._score(net, X, Y)), end=\" \")\n",
    "        if self.verbose > 0:\n",
    "            print()\n",
    "\n",
    "    def _score(self, clf, X, Y):\n",
    "        Y_pred = clf(X)\n",
    "        score = [f1_score(orig, pred)\n",
    "                 for orig, pred in zip((Y.numpy() > self.roc_auc).astype(np.int), \n",
    "                                       Y_pred.detach().numpy().round().astype(np.int))]\n",
    "        \n",
    "        return sum(score) / len(score)\n",
    "\n",
    "    def _batch_generator(self, X, Y):\n",
    "        indexes = np.arange(len(X))\n",
    "        np.random.shuffle(indexes)\n",
    "        for i in range(len(X) // self.batch_size):\n",
    "            x_batch, y_batch = (X[i*self.batch_size: (i+1)*self.batch_size], \n",
    "                                Y[i*self.batch_size: (i+1)*self.batch_size])\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "    def _split_on_languages(self, corpus):\n",
    "        corpuses = {lang: {\"corp\": [], \"ngrams\": set()} for lang in self.languages}\n",
    "        for text_with_labels in corpus:\n",
    "            text = text_with_labels[1]\n",
    "            lang, n_grams = self._get_lang_and_ngrams(text)\n",
    "            corpuses[lang][\"corp\"].append(text_with_labels)\n",
    "            corpuses[lang][\"ngrams\"].update(n_grams)\n",
    "        return corpuses\n",
    "\n",
    "    def _get_lang_and_ngrams(self, text):\n",
    "        return self._get_language(text), self._n_gramm(text)\n",
    "\n",
    "    def _get_language(\n",
    "            self,  # type: Solution\n",
    "            text   # type: str\n",
    "            ):\n",
    "        # type: (...) -> str\n",
    "        languages = dict.fromkeys(self.languages.keys(), 0)\n",
    "        for sym in text:\n",
    "            for lang, letters in self.languages.items():\n",
    "                if sym in letters:\n",
    "                    languages[lang] += 1\n",
    "        return max(languages.items(), key=lambda tup: tup[1])[0]\n",
    "\n",
    "    def _n_gramm(self, text):\n",
    "        # type: (...) -> tp.Tuple[str]\n",
    "        n=self.gramm_size\n",
    "        n_grams = ngrams(\" \".join(text.split()).lower(), n)\n",
    "        for grams in n_grams:\n",
    "            yield grams\n",
    "\n",
    "    def _labels_to_vector(self, labels):\n",
    "        classes = choice(list(labels))\n",
    "        return np.array([bool(class_ in classes) for class_ in self.classes], dtype=np.int)\n",
    "\n",
    "    def _corpus_to_vectors(self, corpus, ngramms):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for corp in corpus:\n",
    "            X.append(self._text_to_vector(corp[1], ngramms))\n",
    "            Y.append(self._labels_to_vector(corp[2].values()))\n",
    "        return (torch.from_numpy(np.array(X)).type(torch.FloatTensor), \n",
    "                torch.from_numpy(np.array(Y)).type(torch.FloatTensor))\n",
    "\n",
    "    def _text_to_vector(self, text, ngramms):\n",
    "        vec = np.zeros(len(ngramms))\n",
    "        for gramm in self._n_gramm(text):\n",
    "            if gramm in ngramms:\n",
    "                vec[ngramms[gramm]] += 1\n",
    "        return (vec / np.linalg.norm(vec))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_classes(classes):\n",
    "        if classes is not None:\n",
    "            return classes\n",
    "        return [\"Угроза\", \"Уязвимость\", \"Эксплойт\", \"Инцидент\", \"Вредоносное ПО\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_non_class(nonclass):\n",
    "        if nonclass is not None:\n",
    "            return nonclass\n",
    "        return \"Прочее\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_languages(languages):\n",
    "        if languages is not None:\n",
    "            return languages\n",
    "        return {\"ru\": set(\"йцукенгшщзхъфывапролджэёячсмитьбю\"),\n",
    "                \"en\": set(\"qwertyuiopasdfghjklzxcvbnm\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 23.17462730407715 \n",
      "epoch: 1 loss: 21.611413955688477 \n",
      "epoch: 2 loss: 21.518016815185547 \n",
      "epoch: 3 loss: 20.15555191040039 \n",
      "epoch: 4 loss: 17.965036392211914 \n",
      "epoch: 5 loss: 15.779212951660156 \n",
      "epoch: 6 loss: 16.009010314941406 \n",
      "epoch: 7 loss: 15.701870918273926 \n",
      "epoch: 8 loss: 15.066007614135742 \n",
      "epoch: 9 loss: 14.687211036682129 \n",
      "epoch: 10 loss: 13.874427795410156 \n",
      "epoch: 11 loss: 13.433914184570312 \n",
      "epoch: 12 loss: 12.428201675415039 \n",
      "epoch: 13 loss: 13.25733757019043 \n",
      "epoch: 14 loss: 12.087092399597168 \n",
      "epoch: 15 loss: 14.77341079711914 \n",
      "epoch: 16 loss: 13.404492378234863 \n",
      "epoch: 17 loss: 15.720309257507324 \n",
      "epoch: 18 loss: 12.269837379455566 \n",
      "epoch: 19 loss: 15.155976295471191 \n",
      "epoch: 20 loss: 13.873270034790039 \n",
      "epoch: 21 loss: 13.944623947143555 \n",
      "epoch: 22 loss: 14.689910888671875 \n",
      "epoch: 23 loss: 14.314431190490723 \n",
      "epoch: 24 loss: 12.814324378967285 \n",
      "epoch: 25 loss: 11.621940612792969 \n",
      "epoch: 26 loss: 12.403376579284668 \n",
      "epoch: 27 loss: 12.069376945495605 \n",
      "epoch: 28 loss: 12.5039701461792 \n",
      "epoch: 29 loss: 11.956992149353027 \n",
      "epoch: 30 loss: 10.69396686553955 \n",
      "epoch: 31 loss: 11.019457817077637 \n",
      "epoch: 32 loss: 12.209488868713379 \n",
      "epoch: 33 loss: 11.228975296020508 \n",
      "epoch: 34 loss: 10.9053955078125 \n",
      "epoch: 35 loss: 11.13848876953125 \n",
      "epoch: 36 loss: 10.388666152954102 \n",
      "epoch: 37 loss: 10.279234886169434 \n",
      "epoch: 38 loss: 9.980267524719238 \n",
      "epoch: 39 loss: 9.879871368408203 \n",
      "epoch: 40 loss: 9.724059104919434 \n",
      "epoch: 41 loss: 9.081314086914062 \n",
      "epoch: 42 loss: 9.366960525512695 \n",
      "epoch: 43 loss: 9.247527122497559 \n",
      "epoch: 44 loss: 8.610503196716309 \n",
      "epoch: 45 loss: 8.394502639770508 \n",
      "epoch: 46 loss: 8.249828338623047 \n",
      "epoch: 47 loss: 8.293667793273926 \n",
      "epoch: 48 loss: 8.33806037902832 \n",
      "epoch: 49 loss: 8.192106246948242 \n",
      "epoch: 50 loss: 7.931175231933594 \n",
      "epoch: 51 loss: 7.913232326507568 \n",
      "epoch: 52 loss: 7.988009929656982 \n",
      "epoch: 53 loss: 7.766549587249756 \n",
      "epoch: 54 loss: 7.654481887817383 \n",
      "epoch: 55 loss: 7.791629314422607 \n",
      "epoch: 56 loss: 7.74060583114624 \n",
      "epoch: 57 loss: 7.425607204437256 \n",
      "epoch: 58 loss: 7.272491931915283 \n",
      "epoch: 59 loss: 7.460948467254639 \n",
      "epoch: 60 loss: 7.568753719329834 \n",
      "epoch: 61 loss: 7.4482951164245605 \n",
      "epoch: 62 loss: 7.250840663909912 \n",
      "epoch: 63 loss: 7.226860523223877 \n",
      "epoch: 64 loss: 7.333922386169434 \n",
      "epoch: 65 loss: 7.313451766967773 \n",
      "epoch: 66 loss: 7.120838165283203 \n",
      "epoch: 67 loss: 7.020779132843018 \n",
      "epoch: 68 loss: 7.086860656738281 \n",
      "epoch: 69 loss: 7.186159610748291 \n",
      "epoch: 70 loss: 7.195150852203369 \n",
      "epoch: 71 loss: 7.0462751388549805 \n",
      "epoch: 72 loss: 6.879879474639893 \n",
      "epoch: 73 loss: 6.889956474304199 \n",
      "epoch: 74 loss: 7.047062397003174 \n",
      "epoch: 75 loss: 7.154343128204346 \n",
      "epoch: 76 loss: 7.056952476501465 \n",
      "epoch: 77 loss: 6.839755058288574 \n",
      "epoch: 78 loss: 6.6978759765625 \n",
      "epoch: 79 loss: 6.707775115966797 \n",
      "epoch: 80 loss: 6.822944164276123 \n",
      "epoch: 81 loss: 6.950550556182861 \n",
      "epoch: 82 loss: 6.984281063079834 \n",
      "epoch: 83 loss: 6.891345977783203 \n",
      "epoch: 84 loss: 6.733921051025391 \n",
      "epoch: 85 loss: 6.611301422119141 \n",
      "epoch: 86 loss: 6.572465896606445 \n",
      "epoch: 87 loss: 6.611525535583496 \n",
      "epoch: 88 loss: 6.6689653396606445 \n",
      "epoch: 89 loss: 6.692677974700928 \n",
      "epoch: 90 loss: 6.656601905822754 \n",
      "epoch: 91 loss: 6.5724945068359375 \n",
      "epoch: 92 loss: 6.477118492126465 \n",
      "epoch: 93 loss: 6.411011219024658 \n",
      "epoch: 94 loss: 6.385905742645264 \n",
      "epoch: 95 loss: 6.385839939117432 \n",
      "epoch: 96 loss: 6.391422748565674 \n",
      "epoch: 97 loss: 6.390427589416504 \n",
      "epoch: 98 loss: 6.373653411865234 \n",
      "epoch: 99 loss: 6.339868545532227 \n",
      "epoch: 0 loss: 15.90950870513916 \n",
      "epoch: 1 loss: 14.21073055267334 \n",
      "epoch: 2 loss: 14.296157836914062 \n",
      "epoch: 3 loss: 14.19555377960205 \n",
      "epoch: 4 loss: 13.294678688049316 \n",
      "epoch: 5 loss: 12.201252937316895 \n",
      "epoch: 6 loss: 10.43391227722168 \n",
      "epoch: 7 loss: 9.8114013671875 \n",
      "epoch: 8 loss: 8.776083946228027 \n",
      "epoch: 9 loss: 10.33249568939209 \n",
      "epoch: 10 loss: 8.077973365783691 \n",
      "epoch: 11 loss: 8.699318885803223 \n",
      "epoch: 12 loss: 9.474349975585938 \n",
      "epoch: 13 loss: 8.03310489654541 \n",
      "epoch: 14 loss: 9.205328941345215 \n",
      "epoch: 15 loss: 11.11072826385498 \n",
      "epoch: 16 loss: 8.760778427124023 \n",
      "epoch: 17 loss: 9.00084114074707 \n",
      "epoch: 18 loss: 10.10550594329834 \n",
      "epoch: 19 loss: 10.123383522033691 \n",
      "epoch: 20 loss: 11.514652252197266 \n",
      "epoch: 21 loss: 9.808392524719238 \n",
      "epoch: 22 loss: 9.425237655639648 \n",
      "epoch: 23 loss: 9.212581634521484 \n",
      "epoch: 24 loss: 9.219537734985352 \n",
      "epoch: 25 loss: 9.418203353881836 \n",
      "epoch: 26 loss: 8.833166122436523 \n",
      "epoch: 27 loss: 7.057713508605957 \n",
      "epoch: 28 loss: 6.661187171936035 \n",
      "epoch: 29 loss: 7.916487693786621 \n",
      "epoch: 30 loss: 8.459041595458984 \n",
      "epoch: 31 loss: 7.585971832275391 \n",
      "epoch: 32 loss: 6.649351119995117 \n",
      "epoch: 33 loss: 6.29351806640625 \n",
      "epoch: 34 loss: 6.328434467315674 \n",
      "epoch: 35 loss: 6.47442102432251 \n",
      "epoch: 36 loss: 6.580426216125488 \n",
      "epoch: 37 loss: 6.640841484069824 \n",
      "epoch: 38 loss: 6.662660598754883 \n",
      "epoch: 39 loss: 6.60389518737793 \n",
      "epoch: 40 loss: 6.479065895080566 \n",
      "epoch: 41 loss: 6.322991847991943 \n",
      "epoch: 42 loss: 6.2233123779296875 \n",
      "epoch: 43 loss: 6.218042373657227 \n",
      "epoch: 44 loss: 6.162441253662109 \n",
      "epoch: 45 loss: 5.983848571777344 \n",
      "epoch: 46 loss: 5.795499801635742 \n",
      "epoch: 47 loss: 5.752892017364502 \n",
      "epoch: 48 loss: 5.777796745300293 \n",
      "epoch: 49 loss: 5.709948539733887 \n",
      "epoch: 50 loss: 5.554809093475342 \n",
      "epoch: 51 loss: 5.472344398498535 \n",
      "epoch: 52 loss: 5.455743789672852 \n",
      "epoch: 53 loss: 5.429960250854492 \n",
      "epoch: 54 loss: 5.327823162078857 \n",
      "epoch: 55 loss: 5.230314254760742 \n",
      "epoch: 56 loss: 5.1689252853393555 \n",
      "epoch: 57 loss: 5.141140937805176 \n",
      "epoch: 58 loss: 5.118573188781738 \n",
      "epoch: 59 loss: 5.073864459991455 \n",
      "epoch: 60 loss: 5.007009029388428 \n",
      "epoch: 61 loss: 4.937326431274414 \n",
      "epoch: 62 loss: 4.8879194259643555 \n",
      "epoch: 63 loss: 4.86607027053833 \n",
      "epoch: 64 loss: 4.852252960205078 \n",
      "epoch: 65 loss: 4.814950942993164 \n",
      "epoch: 66 loss: 4.735136032104492 \n",
      "epoch: 67 loss: 4.633189678192139 \n",
      "epoch: 68 loss: 4.546176433563232 \n",
      "epoch: 69 loss: 4.504909515380859 \n",
      "epoch: 70 loss: 4.496212959289551 \n",
      "epoch: 71 loss: 4.483432769775391 \n",
      "epoch: 72 loss: 4.433345794677734 \n",
      "epoch: 73 loss: 4.3618340492248535 \n",
      "epoch: 74 loss: 4.305887222290039 \n",
      "epoch: 75 loss: 4.275396823883057 \n",
      "epoch: 76 loss: 4.248312950134277 \n",
      "epoch: 77 loss: 4.204100131988525 \n",
      "epoch: 78 loss: 4.151403903961182 \n",
      "epoch: 79 loss: 4.10262393951416 \n",
      "epoch: 80 loss: 4.066848278045654 \n",
      "epoch: 81 loss: 4.0415167808532715 \n",
      "epoch: 82 loss: 4.022531509399414 \n",
      "epoch: 83 loss: 3.99957275390625 \n",
      "epoch: 84 loss: 3.9725759029388428 \n",
      "epoch: 85 loss: 3.9422593116760254 \n",
      "epoch: 86 loss: 3.904916286468506 \n",
      "epoch: 87 loss: 3.8691489696502686 \n",
      "epoch: 88 loss: 3.840250015258789 \n",
      "epoch: 89 loss: 3.818397283554077 \n",
      "epoch: 90 loss: 3.8025574684143066 \n",
      "epoch: 91 loss: 3.786418914794922 \n",
      "epoch: 92 loss: 3.766371488571167 \n",
      "epoch: 93 loss: 3.7427637577056885 \n",
      "epoch: 94 loss: 3.7173967361450195 \n",
      "epoch: 95 loss: 3.692291498184204 \n",
      "epoch: 96 loss: 3.6705307960510254 \n",
      "epoch: 97 loss: 3.6530141830444336 \n",
      "epoch: 98 loss: 3.639411449432373 \n",
      "epoch: 99 loss: 3.6282966136932373 \n",
      "0.66321987278666\n"
     ]
    }
   ],
   "source": [
    "def to_vec(st):\n",
    "    classes = [\"Угроза\", \"Уязвимость\", \"Эксплойт\", \"Инцидент\", \"Вредоносное ПО\", \"Прочее\"]\n",
    "    return [int(elem in st) for elem in classes]\n",
    "\n",
    "indexes = np.arange(len(data))\n",
    "np.random.shuffle(indexes)\n",
    "train_data = [data[i] for i in indexes[:1000]]\n",
    "test_data = [data[i] for i in indexes[1000:]]\n",
    "\n",
    "clf = Solution(verbose=2)\n",
    "clf.train(data_generator(train_data))\n",
    "\n",
    "metrics = []\n",
    "for x, y in test_data_generator(test_data):\n",
    "    y_pred, y_orig = to_vec(clf.predict([x])[0]), to_vec(list(y.values())[0])\n",
    "    metrics.append(f1_score(y_orig, y_pred))\n",
    "print(sum(metrics) / len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = test_data_generator(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6141997593261135\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "def to_vec(st):\n",
    "    classes = [\"Угроза\", \"Уязвимость\", \"Эксплойт\", \"Инцидент\", \"Вредоносное ПО\", \"Прочее\"]\n",
    "    return [int(elem in st) for elem in classes]\n",
    "for x, y in test_data_generator(test_data):\n",
    "    y_pred, y_orig = to_vec(clf.predict([x])[0]), to_vec(list(y.values())[0])\n",
    "    metrics.append(f1_score(y_orig, y_pred))\n",
    "print(sum(metrics) / len(metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
